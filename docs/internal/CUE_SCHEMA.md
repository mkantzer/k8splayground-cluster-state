 Cue Schema

While this repository is responsible for defining, consuming, transforming, and rendering various schema, it also has its own internal data structure that it uses to manage all of that.
This document provides information about that structure.

## Background

Cuelang is not a traditional programming language.
It is not Turing complete, nor is it designed to act as a "function", where it's passed some other input and then runs specific logic on it.
All cue evaluations execute using the same binary, which doesn't contain any information about your data processing.

Rather, it is a _data_ and _configuration_ language. You can think of it like JSON, if you could put rules and types and constraints _in the JSON itself_, instead of just into whatever application creates or consumes it.

It's also a superset of JSON: any JSON data can be trivially represented as cuelang.

Because Cue operates as a data language, all configuration, schema, validation, value, and inference information must be present in the same "workspace" at the same time.
The logic is a _part_ of the data, not a separate entity.

As a result, we do not define our input and output objects directly at the root level.
Rather, we do something akin to namespacing, by nesting those within containing blocks, allowing us to more trivially operate and transform them.

This document serves as a guide to that organization and its subcomponents, as well as explanations of how they are used and the logic behind them.

> NOTE: cuelang is a _general_ purpose_ data language, not a kubernetes-specific one. As such, we define our own schema, logic, and tooling for our specific purposes.

## Root Objects

### `kubernetes:`

The top level `kubernetes:` object holds direct cuelang representations of kubernetes manifests, under a specific structure of sub-objects.
These are organized together to provided simplified access and transformation for all defined objects.

`kubernetes:` also acts as the source from which the YAML stream generated by `cue dump` originates (albeit with an intermediate step. See `k8sObjects:`)

The organization of this top-level object is as follows:

`kubernetes: [context]: [strings.ToLower(Kind)] : [metadata.name]: {<manifest>}`

By grouping by `kind`, we are able to trivially specify that all objects are valid examples of  Kubernetes objects (eg: all the fields in a Deployment are fields that are supposed to be there, and of the correct type).
We can also put in system-wide required configuration or defaults (every pod must run as non-root, pods without resource requests or limits get defaults automatically).

By further grouping by object name, we can do searches and validations that require access to a specific object, without needing any list comprehension filters.
There may be cases where we need to filter by something other than name, but this is likely the most common.

### `mesoservice:`

This object contains any definitions for an abstraction layer around kubernetes manifests.
You can think of it kind of like the `values.yml` of a helm chart: it's a simplified structured input used to programmatically generate kubernetes manifests.

The top level `mesoservice:` object directly contains the `#Mesoservice` schema (see `schema/mesoservice/<some version>/interface.cue` for the actual spec).
Note that this object _by itself_ does not generate objects in the `kubernetes:` top-level object, but is instead _just_ a definition. See `appGenerator:`

### `appGenerator:`

This object uses the `mesoservice:` object to generate kubernetes objects. It uses the [function pattern](https://cuetorials.com/patterns/functions/) for this.
It outputs a field `appGenerator.out.kubernetes`, which is formatted exactly like the top-level `kubernetes:` object, and used to add entries to same, as:
```
appGenerator: mesoservice_v1.#Generator & {in: {
  metadata: mesoservice.metadata
  spec:     mesoservice.spec
}}
kubernetes: {
	appGenerator.out.kubernetes
	...
}
```

### `k8sObjects:`:

This object essentially loops over `kubernetes:`'s nested fields `[kind]` and `[name]` to produce a flat list of generated objects, organized by context, that can be passed directly into `yaml.MarshalStream(k8sObjects.<context>)`.

Its implementation is simple:
```
for contextName, contextValue in kubernetes {
	k8sObjects: "\(contextName)": [
		for kind in contextValue
		for object in kind {object},
	]
}
```

### `inputTags:`:

This versioned object manages dynamic input values. See [defs_tags.cue](../defs_tags.cue) for additional information.

## Contexts

Many parts of our schema reference `Context`s: these represent the different transformations made to your `mesoservice` configuration for different uses.
This is meant to be almost entirely transparent to the end user: it's primarily used for generating platform-centric resources.

See [PLATFORM.md](./PLATFORM.md#design-philosophy-and-intent) for an example of how we might use contexts.

The currently **implemented** contexts are:
- `prime`: the objects required to actually execute a mesoservice (deployments, services, httproutes, etc.). Sent to clusters that will host the compute. Does not include backing components.
- `onboard`: generates the Argo Applications and ApplicationSets that actually use the above contexts.

The currently **proposed** contexts are:
- `workflow`: this would generate an app's Argo Workflow Templates, and associated objects.
- `cloudresources`: any required Config Connector objects. Sent to the dedicated Config Connector cluster
- `qa`: any components required in a QA environment. Does _not_ replace `prime`: they are intended to be used _in parallel_.
- `smoke`: any components required in a smoke environment. Does _not_ replace `prime`: they are intended to be used _in parallel_.

The following contexts are purely "would be cool one day":
- `mock`: consumed by other smoke envs (mechanism TBD), this renders minimum-viable config for standing up an app's mocks. Ideally, it's just networking and a really small container. If an app doesn't _have_ mocks, it'll point to the QA cluster.
- `local`: eventually, we may build out systems to help devs run their apps in local k8s clusters, using tools like skaffold and kind. This would render their app specifically configured for that environment.
